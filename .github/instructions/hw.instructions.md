---
applyTo: "**"
---

## 題目
請以 Bank 為範例進行資料前處理（由以下講義的技巧來進行）。整理後的資料，請以 CSV 的格式輸出（自己找答案）。

## 方法
您好，既然您要求**先去除手動選擇特徵**的步驟，我們將專注於應用「課本介紹的機器學習技巧」來進行特徵選擇（Feature Selection）和降維（Dimensionality Reduction）。

這表示我們必須先處理原始資料中的**所有特徵**，然後才能使用演算法或統計方法來判斷哪些特徵是重要的。

以下是您應當採取的規劃步驟：

---

### 第一部分：資料前處理與技巧性特徵選擇 (Technical Feature Selection)

由於您不再手動移除特徵（如 `job`, `marital`, `education` 等），您必須針對原始資料集中的所有欄位進行清理和轉換。

#### 步驟 1：載入與預處理所有特徵

1.  **資料載入：** 載入 `bank.csv` 檔案。
2.  **處理類別資料（Categorical Data）：** 機器學習模型只能處理數字。因此，所有類別變數都必須數值化。
    *   **目標變數 $y$ (deposit) 和二元特徵 (loan)：** 將二元的類別標籤（如「yes」和「no」）轉換為 0 或 1。
    *   **多類別特徵（如 job, marital）：** 對於具有多個離散值的特徵，必須使用 **One-Hot Encoding** 或類似的技術（如 `LabelBinarizer`）將其轉換成數值格式，以避免模型誤認為這些類別之間有順序關係。
3.  **處理遺失值（Missing Values）：** 檢查資料中是否存在遺失值（null values）。若有，需要決定處理策略，例如使用 `Imputer` 類別，以平均值 (mean)、中位數 (median) 或頻率 (frequency) 填補。
4.  **標準化與正規化（Scaling and Normalization）：** 為了避免不同特徵的尺度差異影響模型性能（尤其是線性模型和基於距離的演算法），必須對數值特徵進行標準化。使用 `StandardScaler` 將特徵轉換為零均值和單位變異數。

#### 步驟 2：應用課本技巧進行特徵選擇/降維

在所有特徵都已數值化並標準化之後，您需要選擇並應用至少一種技術來減少特徵的數量，以獲得優化的資料集。

**操作選擇（擇一或多個）：**

1.  **主成分分析 (Principal Component Analysis, PCA)：**
    *   **原理：** PCA 是一種降維技術，它提取一組新的變數（主成分），這些新變數是原始變數的線性組合，旨在保留資料中最大部分的變異度。
    *   **執行：** 使用 `sklearn.decomposition.PCA`，並決定要保留多少主成分（例如，保留能解釋 90% 變異度的 $K$ 個主成分）。

2.  **Lasso 迴歸（Lasso Regression, L1 正規化）：**
    *   **原理：** Lasso 在損失函數中引入 L1 懲罰項，這會強制將不相關特徵的係數壓縮為零，從而達到特徵選擇的效果，產生一個稀疏模型 (sparse model)。
    *   **執行：** 使用 `LassoCV` 訓練模型，然後利用 `SelectFromModel` 根據訓練後係數不為零的特徵來選擇最終的特徵子集。

3.  **過濾器方法（Filter-based Methods）：**
    *   **低變異度過濾器 (Low Variance Filter)：** 計算所有特徵的變異數，並移除變異數低於預設閾值的特徵。
    *   **高相關性過濾器 (High Correlation Filter)：** 檢查特徵之間的相關係數。如果兩個特徵高度相關（例如 $> 0.5$ 或 $0.6$），則移除其中一個，以避免冗餘。
    *   **基於統計檢定的特徵選擇：** 使用 `SelectKBest` 搭配 `f_regression`（或 `chi2` 用於分類）來選擇與目標變數相關性最高的 $K$ 個特徵。

#### 步驟 3：資料輸出與分割

1.  **資料輸出：** 將經過特徵選擇/降維（步驟 2）後所得到的特徵集，連同目標變數 $y$（deposit），輸出為一個 CSV 檔案（例如 `bank_tech_features.csv`）。
2.  **資料分割：** 使用 `train_test_split` 函式，將這個新的資料集分割為訓練集和測試集 (`X_train`, `X_test`, `y_train`, `y_test`)，建議比例為 8/2 或 7/3。

---

### 第二部分：使用線性迴歸進行預測 (Linear Regression Prediction)

使用經過「技巧性選擇」後的資料集（步驟 3 的訓練/測試集），進行線性迴歸分析。

#### 1. 純線性機制預測

這是指在不對特徵進行多項式擴展的情況下，直接使用線性模型。

*   **模型選擇：** 使用 `LinearRegression` 或 Ridge/Lasso 迴歸。
*   **訓練與評估：** 使用 `X_train` 和 `y_train` 訓練模型 (`fit`)，並在 `X_test` 上評估模型的效能（例如使用 $R^2$ 分數 `score`）。
*   **離群值處理（可選）：** 如果您懷疑資料中存在離群值，可以使用 **RANSACRegressor** 結合 `LinearRegression`，以避免離群值對係數的偏差影響。

#### 2. 非線性機制預測（多項式迴歸）

要讓線性模型具備處理非線性資料的能力，必須使用多項式特徵轉換（Polynomial Regression）的技巧。

*   **特徵擴展：** 使用 `PolynomialFeatures` 類別對您的特徵集 (`X_train`) 進行擴展。例如，設定 `degree=2`，這會將原始特徵轉換為包含二次項和交叉項的新特徵集。
*   **訓練與評估：**
    *   使用擴展後的訓練集 (`Xp_train`)，訓練一個新的 `LinearRegression` 模型。
    *   對測試集 (`X_test`) 執行相同的多項式轉換，然後用訓練好的模型進行預測和評估。